/*
分配socket并绑定某个线程
schedule(socket) {
    Task(socket) {
        swap(socket)
    } task;
    socket已经不可能在本线程析构了
    task_list.push_back(task);
}
*/
Socket::ptr new_sock = nullptr;
try {
    LOCK_GUARD(m_event_cb_mutex);
    new_sock = m_on_before_accept_cb(m_poller);
} catch (std::exception &e) {
    HAMMER_LOG_WARN(g_logger) << "Exception occurred when on_before_accept: " << e.what();
    close(fd);
    continue;
}
if (!new_sock) {
    new_sock = Socket::createSocket(m_poller, false);
}

//std::shared_ptr<void> async_accept(nullptr, [new_sock, new_sock_fd](void *) {
auto async_accept = []() {
    auto new_sock_fd = new_sock->setSocketFD(fd);
    completed();
    try {
        LOCK_GUARD(m_event_cb_mutex);
        m_on_accept_cb(new_sock, completed);
    } catch (std::exception &e) {
        HAMMER_LOG_WARN(g_logger) << "Exception occurred when emit onAccept: " << e.what();
        continue;
    }
}



m_on_accept_cb(new_sock, completed); 中
如果只将completed"断舍离" 而new_sock没有"断舍离" 那么仍会有new_sock在accept线程析构的可能
必须将accept线程中 所有的关于socket的引用都"断舍离"
---注意这里的"断舍离"意思是 lambda捕获表达式的那种(boost源码中见到的c++14)

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
关于设计

关于新连接读写监听 总体上采用非"批量读写(ats那种 针对读写有ready队列)"模式

相对来说muduo的设计 分层(channel->connection 然后acceptor/server/client)更好一些 更清晰一些
ats分层:    | ip | fd buffer | ssl |
muduo分层:  channel:fd + event + cb | connection: ip socket buffer |
hammer基本上没有分层 直接一个socket 得益于lambda 直接把weak_sock放到"可调对象"捕捉参数里 回调时直接可以拿到这个weak_sock 
    将其还原成strong_sock 进行业务处理 从而绕过channel这一层直接拿到socket

hammer暴露给上层的回调传参很简单 基本上参数中有buffer即可  eg: onReadCB 

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Architecture:        x86_64
CPU op-mode(s):      32-bit, 64-bit
Byte Order:          Little Endian
CPU(s):              4
On-line CPU(s) list: 0-3
Thread(s) per core:  1
Core(s) per socket:  2
Socket(s):           2
NUMA node(s):        1
Vendor ID:           GenuineIntel
BIOS Vendor ID:      GenuineIntel
CPU family:          6
Model:               158
Model name:          Intel(R) Core(TM) i5-9500 CPU @ 3.00GHz
BIOS Model name:     Intel(R) Core(TM) i5-9500 CPU @ 3.00GHz
Stepping:            10
CPU MHz:             3000.003
BogoMIPS:            6000.00
Virtualization:      VT-x
Hypervisor vendor:   VMware
Virtualization type: full
L1d cache:           32K
L1i cache:           32K
L2 cache:            256K
L3 cache:            9216K
NUMA node0 CPU(s):   0-3
Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single pti ssbd ibrs ibpb stibp tpr_shadow vnmi ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 invpcid rtm mpx rdseed adx smap clflushopt xsaveopt xsavec xsaves arat flush_l1d arch_capabilities


-O3 -jemalloc

单线程
accept绑定线程 
----------------hammer-------------------
Server Software:        
Server Hostname:        0.0.0.0
Server Port:            9527

Document Path:          /
Document Length:        0 bytes

Concurrency Level:      100
Time taken for tests:   40.693 seconds
Complete requests:      1000000
Failed requests:        0
Total transferred:      19000000 bytes
HTML transferred:       0 bytes
Requests per second:    24574.10 [#/sec] (mean)
Time per request:       4.069 [ms] (mean)
Time per request:       0.041 [ms] (mean, across all concurrent requests)
Transfer rate:          455.96 [Kbytes/sec] received

Connection Times (ms)
              min  mean[+/-sd] median   max
Connect:        0    2   0.3      2      14
Processing:     1    2   0.4      2      15
Waiting:        0    2   0.5      2      15
Total:          2    4   0.4      4      17

Percentage of the requests served within a certain time (ms)
  50%      4
  66%      4
  75%      4
  80%      4
  90%      4
  95%      5
  98%      5
  99%      5
 100%     17 (longest request)

----------------ngx------------------------
Server Software:        nginx/1.16.1
Server Hostname:        0.0.0.0
Server Port:            9527

Document Path:          /
Document Length:        0 bytes

Concurrency Level:      100
Time taken for tests:   46.014 seconds
Complete requests:      1000000
Failed requests:        0
Total transferred:      156000000 bytes
HTML transferred:       0 bytes
Requests per second:    21732.45 [#/sec] (mean)
Time per request:       4.601 [ms] (mean)
Time per request:       0.046 [ms] (mean, across all concurrent requests)
Transfer rate:          3310.80 [Kbytes/sec] received

Connection Times (ms)
              min  mean[+/-sd] median   max
Connect:        0    2   0.4      2      15
Processing:     1    3   0.5      3      15
Waiting:        0    2   0.6      2      15
Total:          3    5   0.4      5      17

Percentage of the requests served within a certain time (ms)
  50%      5
  66%      5
  75%      5
  80%      5
  90%      5
  95%      5
  98%      5
  99%      6
 100%     17 (longest request)

双线程跑不过nginx nginx是18000 hammer是15000(accept绑定线程即return nullptr) 16000(随机调度)


----------------------------------------
正常的读写关闭回调
2023-01-08 22:21:56     187943  0       [WARN]  [system]        /root/CLionProjects/local_git/hammer/hammer/socket.cc:398       attachEvent: 3
2023-01-08 22:21:56     187943  0       [WARN]  [system]        /root/CLionProjects/local_git/hammer/hammer/socket.cc:400       attachEvent r: 3
2023-01-08 22:21:56     187943  0       [WARN]  [system]        /root/CLionProjects/local_git/hammer/hammer/tcp_server.cc:149   before server read0
2023-01-08 22:21:56     187943  0       [WARN]  [system]        /root/CLionProjects/local_git/hammer/hammer/tcp_server.cc:162   after server write
2023-01-08 22:21:56     187943  0       [WARN]  [system]        /root/CLionProjects/local_git/hammer/hammer/tcp_server.cc:194   after server err
2023-01-08 22:21:56     187943  0       [WARN]  [system]        /root/CLionProjects/local_git/hammer/hammer/tcp_server.cc:151   after server read0
2023-01-08 22:21:56     187943  0       [WARN]  [system]        /root/CLionProjects/local_git/hammer/hammer/socket.cc:405       attachEvent w: 3


异常的'不注册写回调': 在发完数据后 回调上层时发生异常
2023-01-08 22:24:25     188129  0       [WARN]  [system]        /root/CLionProjects/local_git/hammer/hammer/socket.cc:398       attachEvent: 3
2023-01-08 22:24:25     188129  0       [WARN]  [system]        /root/CLionProjects/local_git/hammer/hammer/socket.cc:400       attachEvent r: 3
2023-01-08 22:24:25     188129  0       [WARN]  [system]        /root/CLionProjects/local_git/hammer/hammer/tcp_server.cc:149   before server read0
2023-01-08 22:24:25     188129  0       [WARN]  [system]        /root/CLionProjects/local_git/hammer/hammer/tcp_server.cc:156   after server read2
2023-01-08 22:24:25     188129  0       [WARN]  [system]        /root/CLionProjects/local_git/hammer/hammer/tcp_server.cc:194   after server err
2023-01-08 22:24:25     188129  0       [WARN]  [system]        /root/CLionProjects/local_git/hammer/hammer/socket.cc:405       attachEvent w: 3

对比来看正常的就是多了一个上层写回调 性能从29000降到240000 cpu从88到64 找不到原因

---------------------------------------
服务端如何优雅的关闭: 即如何确保响应完全发给了客户端? 
当然当前这种 全塞到了缓冲区的方式 也无可厚非 但是仍会收到solinger影响
应该设计出一种机制 不受solinger影响 


